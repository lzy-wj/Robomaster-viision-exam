# -*- coding: utf-8 -*-
"""
CIFAR-10 è®­ç»ƒè„šæœ¬ - ç®€æ´ç¨³å®šç‰ˆæœ¬
"""

import os
# ä¼˜åŒ–å¤šGPUé€šä¿¡ï¼Œå¿…é¡»åœ¨torchå¯¼å…¥ä¹‹å‰è®¾ç½®
# ç¦ç”¨P2Pé€šä¿¡ï¼Œå› ä¸ºæ··åˆGPUï¼ˆå¦‚ A6000 å’Œ 3090ï¼‰é€šå¸¸ä¸æ”¯æŒ
os.environ['NCCL_P2P_DISABLE'] = '1'
# ç¦ç”¨InfiniBandï¼Œé€šå¸¸åœ¨å•æœºå¤šå¡æˆ–ä»¥å¤ªç½‘ç¯å¢ƒä¸­ä¸æ˜¯å¿…éœ€çš„ï¼Œæœ‰æ—¶ä¼šå¼•èµ·åˆå§‹åŒ–é—®é¢˜
os.environ['NCCL_IB_DISABLE'] = '1'

import argparse
import os
from pathlib import Path
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

from models import SmallCifarNet, PowerCifarNet


def auto_detect_hardware():
    """è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶é…ç½®å¹¶è¿”å›ä¼˜åŒ–å‚æ•°"""
    config = {
        'device': 'cpu',
        'num_gpus': 0,
        'total_memory_gb': 0,
        'recommended_batch_size': 128,
        'recommended_workers': 4,
        'use_amp': False,
        'use_dataparallel': False
    }
    
    if torch.cuda.is_available():
        config['device'] = 'cuda'
        config['num_gpus'] = torch.cuda.device_count()
        
        # è·å–ç¬¬ä¸€ä¸ªGPUçš„å†…å­˜ä¿¡æ¯
        props = torch.cuda.get_device_properties(0)
        config['total_memory_gb'] = props.total_memory / (1024**3)
        
        print(f"[ç¡¬ä»¶æ£€æµ‹] GPUæ•°é‡: {config['num_gpus']}")
        for i in range(config['num_gpus']):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"[ç¡¬ä»¶æ£€æµ‹] GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # è®¡ç®—æ€»æ˜¾å­˜ï¼ˆæ‰€æœ‰GPUï¼‰
        total_memory_gb = sum(torch.cuda.get_device_properties(i).total_memory for i in range(config['num_gpus'])) / (1024**3)
        print(f"[ç¡¬ä»¶æ£€æµ‹] æ€»æ˜¾å­˜: {total_memory_gb:.1f}GB")
        
        # æ ¹æ®æ€»æ˜¾å­˜å’ŒGPUæ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆæ›´ä¿å®ˆçš„ç­–ç•¥ï¼‰
        if total_memory_gb >= 60:  # 60GB+æ€»æ˜¾å­˜ (3x RTX 3090)
            config['recommended_batch_size'] = 512  # ä¿å®ˆä¸€äº›
            config['recommended_workers'] = 16
        elif total_memory_gb >= 40:  # 40-60GBæ€»æ˜¾å­˜
            config['recommended_batch_size'] = 384
            config['recommended_workers'] = 12
        elif total_memory_gb >= 20:  # 20-40GBæ€»æ˜¾å­˜
            config['recommended_batch_size'] = 256
            config['recommended_workers'] = 8
        else:  # <20GBæ€»æ˜¾å­˜
            config['recommended_batch_size'] = 128
            config['recommended_workers'] = 4
        
        # å¯ç”¨æ··åˆç²¾åº¦å’Œå¤šGPU
        config['use_amp'] = True
        config['use_dataparallel'] = config['num_gpus'] > 1
        
        print(f"[ç¡¬ä»¶æ£€æµ‹] æ¨èæ‰¹æ¬¡å¤§å°: {config['recommended_batch_size']}")
        print(f"[ç¡¬ä»¶æ£€æµ‹] æ¨èworkeræ•°: {config['recommended_workers']}")
    else:
        print("[ç¡¬ä»¶æ£€æµ‹] æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    return config


def find_optimal_batch_size(model, device, start_batch_size=128, max_batch_size=4096):
    """è‡ªåŠ¨å¯»æ‰¾æœ€å¤§å¯ç”¨æ‰¹æ¬¡å¤§å°"""
    model.train()
    batch_size = start_batch_size
    
    print(f"[æ‰¹æ¬¡ä¼˜åŒ–] å¼€å§‹å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°ï¼Œèµ·å§‹: {batch_size}")
    
    while batch_size <= max_batch_size:
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_input = torch.randn(batch_size, 3, 32, 32).to(device)
            test_target = torch.randint(0, 10, (batch_size,)).to(device)
            
            # æ¸…ç©ºç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda') if torch.cuda.is_available() else torch.no_grad():
                output = model(test_input)
                loss = torch.nn.functional.cross_entropy(output, test_target)
                loss.backward()
            
            print(f"[æ‰¹æ¬¡ä¼˜åŒ–] æ‰¹æ¬¡å¤§å° {batch_size} å¯è¡Œ")
            
            # å°è¯•æ›´å¤§çš„æ‰¹æ¬¡
            if batch_size < max_batch_size:
                batch_size = min(batch_size * 2, max_batch_size)
            else:
                break
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                # å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªæˆåŠŸçš„æ‰¹æ¬¡å¤§å°
                batch_size = batch_size // 2
                print(f"[æ‰¹æ¬¡ä¼˜åŒ–] å†…å­˜ä¸è¶³ï¼Œæœ€ç»ˆæ‰¹æ¬¡å¤§å°: {batch_size}")
                break
            else:
                raise e
        except Exception as e:
            print(f"[æ‰¹æ¬¡ä¼˜åŒ–] å‡ºç°é”™è¯¯: {e}")
            batch_size = batch_size // 2
            break
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return max(batch_size, 32)  # æœ€å°æ‰¹æ¬¡å¤§å°ä¸º32


def get_dataloaders(data_dir, batch_size, num_workers=4):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # CIFAR-10 æ ‡å‡†åŒ–å‚æ•°
    normalize = transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2023, 0.1994, 0.2010]
    )
    
    # é€‚åº¦æ•°æ®å¢å¼º - å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        # åªä¿ç•™è½»é‡çº§å¢å¼ºï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–
        transforms.ToTensor(),
        normalize,
        transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3)),  # å‡å°‘æ“¦é™¤å¼ºåº¦
    ])
    
    # æµ‹è¯•æ—¶ä¸å¢å¼º
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        normalize,
    ])

    # æ£€æŸ¥æœ¬åœ°æ•°æ®æ˜¯å¦å­˜åœ¨
    data_path = Path(data_dir)
    cifar_path = data_path / 'cifar-10-batches-py'
    download = not cifar_path.exists()
    
    if not download:
        print(f"[ä¿¡æ¯] å‘ç°æœ¬åœ°æ•°æ®é›†: {cifar_path}")
    else:
        print(f"[ä¿¡æ¯] å¼€å§‹ä¸‹è½½CIFAR-10æ•°æ®é›†åˆ°: {data_path}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = datasets.CIFAR10(
        root=data_dir, train=True, download=download, transform=train_transform
    )
    test_dataset = datasets.CIFAR10(
        root=data_dir, train=False, download=False, transform=test_transform
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - é«˜æ€§èƒ½é…ç½®
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,  # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=4 if num_workers > 0 else 2,  # é¢„å–æ•°æ®
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=4 if num_workers > 0 else 2,
    )
    
    return train_loader, test_loader


@torch.no_grad()
def evaluate(model, test_loader, device):
    """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
    model.eval()
    correct = 0
    total = 0
    
    for images, labels in test_loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    return correct / total


def train_model(args):
    """ä¸»è®­ç»ƒå‡½æ•° - ä¼˜åŒ–GPUé€‰æ‹©å’Œå¯åŠ¨æµç¨‹"""
    print("--- [1/10] è®­ç»ƒå¯åŠ¨ ---")
    
    # è‡ªåŠ¨ç¡¬ä»¶æ£€æµ‹
    hw_config = auto_detect_hardware()
    device = torch.device(hw_config['device'])
    print(f"--- [2/10] ç¡¬ä»¶æ£€æµ‹å®Œæˆ, é»˜è®¤ä¸»è®¾å¤‡: {device} ---")
    
    # ç”¨æˆ·å‚æ•°å¤„ç†...
    if not hasattr(args, 'auto_optimized') or args.auto_optimized:
        if args.batch_size == 128:
            args.batch_size = hw_config['recommended_batch_size']
        if args.num_workers == 4:
            args.num_workers = hw_config['recommended_workers']
    
    # GPUä¼˜åŒ–è®¾ç½®
    if hw_config['device'] == 'cuda':
        torch.backends.cudnn.benchmark = True
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºæ¨¡å‹
    # ... (æ¨¡å‹é€‰æ‹©é€»è¾‘ä¸ä¹‹å‰ç›¸åŒ)
    if hw_config['num_gpus'] >= 3 and hw_config['total_memory_gb'] >= 60:
        model = PowerCifarNet(num_classes=10, width_mult=2.5)
        print("[æ¨¡å‹é€‰æ‹©] ä½¿ç”¨PowerCifarNet (å¤§æ¨¡å‹)")
    elif hw_config['num_gpus'] >= 2 or hw_config['total_memory_gb'] >= 40:
        model = PowerCifarNet(num_classes=10, width_mult=1.5)
        print("[æ¨¡å‹é€‰æ‹©] ä½¿ç”¨PowerCifarNet (ä¸­ç­‰æ¨¡å‹)")
    else:
        model = SmallCifarNet(num_classes=10)
        print("[æ¨¡å‹é€‰æ‹©] ä½¿ç”¨SmallCifarNet (è½»é‡æ¨¡å‹)")
    
    # å…ˆä¸ç§»åŠ¨æ¨¡å‹åˆ°GPUï¼Œç­‰å¾…ç¡®å®šæœ€ç»ˆè®¾å¤‡
    print(f"--- [3/10] æ¨¡å‹ '{type(model).__name__}' åœ¨CPUä¸Šåˆ›å»ºå®Œæˆ ---")

    # æ™ºèƒ½å¤šGPUå¹¶è¡Œ - è‡ªåŠ¨é€‰æ‹©åŒç§æœ€å¤šçš„GPUç»„åˆ
    if hw_config['use_dataparallel']:
        gpu_names = [torch.cuda.get_device_name(i) for i in range(hw_config['num_gpus'])]
        from collections import Counter
        gpu_counts = Counter(gpu_names)
        
        if gpu_counts:
            most_common_gpu = gpu_counts.most_common(1)[0][0]
            device_ids = [i for i, name in enumerate(gpu_names) if name == most_common_gpu]
            
            if len(device_ids) > 1:
                print(f"[GPUä¼˜åŒ–] è‡ªåŠ¨é€‰æ‹©ç»„åˆ: {most_common_gpu} (å…± {len(device_ids)} å¼ )")
                
                # æ ¸å¿ƒä¿®å¤ï¼šè®¾ç½®ä¸»è®¾å¤‡ä¸ºå¡ç»„çš„ç¬¬ä¸€ä¸ªï¼Œå¹¶å°†æ¨¡å‹ç§»åˆ°è¯¥å¡
                device = torch.device(f"cuda:{device_ids[0]}")
                model.to(device)
                print(f"--- [4/10] ä¸»è®¾å¤‡å˜æ›´ä¸º {device}, æ¨¡å‹å·²ç§»è‡³è¯¥GPU ---")
                
                model = nn.DataParallel(model, device_ids=device_ids)
                print(f"--- [5/10] DataParallel æ¨¡å‹åŒ…è£…å®Œæˆ, ä½¿ç”¨è®¾å¤‡: {device_ids} ---")
            else:
                print(f"[GPUä¼˜åŒ–] æœªæ‰¾åˆ°å¤šå¼ åŒç§GPUï¼Œä½¿ç”¨é»˜è®¤å•GPU {device} è®­ç»ƒã€‚")
                model.to(device) # ç§»åŠ¨åˆ°é»˜è®¤è®¾å¤‡
        else:
            print(f"[GPUä¼˜åŒ–] æœªæ‰¾åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒã€‚") # CPUåœºæ™¯
    else:
        model.to(device) # å•GPUåœºæ™¯
        print(f"--- [4/10] å•GPUæ¨¡å¼, æ¨¡å‹å·²ç§»è‡³ {device} ---")

    # ç§»é™¤è‡ªåŠ¨æ‰¹æ¬¡æŸ¥æ‰¾åŠŸèƒ½ä»¥æé«˜å¯åŠ¨é€Ÿåº¦å’Œç¨³å®šæ€§
    print("[ä¼˜åŒ–] å·²ç§»é™¤è‡ªåŠ¨æ‰¹-å¤§å°æŸ¥æ‰¾åŠŸèƒ½ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šbatch-sizeã€‚")

    param_count = sum(p.numel() for p in model.parameters())
    print(f"--- [6/10] æ¨¡å‹å‚æ•°é‡: {param_count:,} ---")

    print("--- [7/10] å‡†å¤‡åŠ è½½æ•°æ® ---")
    train_loader, test_loader = get_dataloaders(args.data_dir, args.batch_size, args.num_workers)
    print("--- [8/10] Dataloaders åˆ›å»ºå®Œæˆ ---")

    # ... (ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨é€»è¾‘ä¸ä¹‹å‰ç›¸åŒ)
    # æŸå¤±å‡½æ•° ...
    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
    # å­¦ä¹ ç‡ç¼©æ”¾ ...
    base_lr = args.lr
    # ...
    scaled_lr = base_lr * 2.0 # simplified
    optimizer = optim.SGD(model.parameters(), lr=scaled_lr, momentum=0.9, nesterov=True, weight_decay=args.weight_decay)
    # è°ƒåº¦å™¨ ...
    if args.batch_size >= 384:
        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=scaled_lr * 1.5, epochs=args.epochs, steps_per_epoch=len(train_loader))
        scheduler_type = 'OneCycleLR'
    else:
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)
        scheduler_type = 'CosineAnnealingLR'

    print("--- [9/10] ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å®Œæˆ ---")

    # æ¢å¤å®Œæ•´çš„è®­ç»ƒå¾ªç¯
    print("--- [10/10] å¼€å§‹è¿›å…¥è®­ç»ƒå¾ªç¯... ---")
    best_acc = 0.0
    train_losses = []
    train_accs = []
    test_accs = []
    
    # åˆå§‹åŒ–AMP scalerï¼ˆå¦‚æœä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
    scaler = torch.amp.GradScaler('cuda') if hw_config['use_amp'] else None

    for epoch in range(1, args.epochs + 1):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            # å…³é”®ï¼šç¡®ä¿æ•°æ®å‘é€åˆ°æ­£ç¡®çš„ä¸»è®¾å¤‡
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            if hw_config['use_amp'] and scaler is not None:
                with torch.amp.autocast('cuda'):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if scheduler_type == 'OneCycleLR':
                scheduler.step()

            running_loss += loss.item()
            with torch.no_grad():
                _, predicted = torch.max(outputs, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            
            print_freq = max(10, len(train_loader) // 20)
            if batch_idx % print_freq == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  Epoch {epoch} [{batch_idx:4d}/{len(train_loader)}] Loss: {loss.item():.4f} | LR: {current_lr:.6f}")

        train_loss = running_loss / len(train_loader)
        train_acc = correct_train / total_train
        test_acc = evaluate(model, test_loader, device)

        if scheduler_type != 'OneCycleLR':
            scheduler.step()
        
        current_lr = optimizer.param_groups[0]['lr']
        gpu_memory = ""
        if torch.cuda.is_available():
            # DataParallel ä¼šå°†æ˜¾å­˜åˆ†æ•£ï¼Œæˆ‘ä»¬åªå…³å¿ƒä¸»è®¾å¤‡çš„
            memory_used = torch.cuda.memory_allocated(device) / 1024**3
            memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3
            gpu_memory = f" | GPU-{device.index}: {memory_used:.1f}/{memory_total:.1f}GB"

        print(f"Epoch {epoch:3d}/{args.epochs} | Loss: {train_loss:.4f} | Train: {train_acc:.4f} | Test: {test_acc:.4f} | LR: {current_lr:.6f}{gpu_memory}")

        if test_acc > best_acc:
            best_acc = test_acc
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_acc': best_acc,
                'args': vars(args)
            }, out_dir / 'best_model.pth')
            print(f"  [ä¿å­˜] æ–°çš„æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {best_acc:.4f}")
        
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_accs.append(test_acc)

    print(f"[å®Œæˆ] è®­ç»ƒç»“æŸï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history = {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'test_accs': test_accs,
        'best_acc': best_acc,
        'final_epoch': args.epochs,
        'config': {
            'batch_size': args.batch_size,
            'lr': args.lr,
            'epochs': args.epochs,
            'model': type(model).__name__ if not isinstance(model, nn.DataParallel) else type(model.module).__name__,
            'hardware': hw_config
        }
    }
    
    with open(out_dir / 'training_history.json', 'w') as f:
        json.dump(history, f, indent=2)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
        plt.title('è®­ç»ƒæŸå¤±å˜åŒ–')
        plt.xlabel('è½®æ•°')
        plt.ylabel('æŸå¤±')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 3, 2)
        plt.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.8)
        plt.plot(test_accs, label='æµ‹è¯•å‡†ç¡®ç‡', alpha=0.8)
        plt.title('å‡†ç¡®ç‡å˜åŒ–')
        plt.xlabel('è½®æ•°')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        plt.subplot(1, 3, 3)
        # é‡æ–°è®¡ç®—å­¦ä¹ ç‡å˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        lr_history = []
        temp_optimizer = optim.SGD([torch.tensor(0., requires_grad=True)], lr=args.lr)
        if scheduler_type == 'OneCycleLR':
            temp_scheduler = optim.lr_scheduler.OneCycleLR(temp_optimizer, max_lr=args.lr * 2.0, epochs=args.epochs, steps_per_epoch=len(train_loader))
            for ep in range(args.epochs):
                for _ in range(len(train_loader)):
                    lr_history.append(temp_optimizer.param_groups[0]['lr'])
                    temp_scheduler.step()
        else:
            temp_scheduler = optim.lr_scheduler.CosineAnnealingLR(temp_optimizer, T_max=args.epochs, eta_min=args.min_lr)
            for ep in range(args.epochs):
                lr_history.append(temp_optimizer.param_groups[0]['lr'])
                temp_scheduler.step()
        
        if scheduler_type == 'OneCycleLR':
            plt.plot(lr_history, alpha=0.8)
            plt.title('å­¦ä¹ ç‡å˜åŒ–ï¼ˆæ‰¹æ¬¡çº§ï¼‰')
            plt.xlabel('æ‰¹æ¬¡')
        else:
            plt.plot(lr_history, alpha=0.8)
            plt.title('å­¦ä¹ ç‡å˜åŒ–ï¼ˆè½®æ¬¡çº§ï¼‰')
            plt.xlabel('è½®æ•°')
        plt.ylabel('å­¦ä¹ ç‡')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(out_dir / 'training_curves.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"[ä¿¡æ¯] è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {out_dir / 'training_curves.png'}")
        
    except ImportError:
        print("[æç¤º] æœªå®‰è£…matplotlibï¼Œè·³è¿‡ç»˜å›¾")
    except Exception as e:
        print(f"[è­¦å‘Š] ç»˜å›¾æ—¶å‡ºç°é”™è¯¯: {e}")
    
    print(f"[ä¿¡æ¯] è®­ç»ƒå†å²å·²ä¿å­˜: {out_dir / 'training_history.json'}")
    return best_acc


def main():
    parser = argparse.ArgumentParser(description='CIFAR-10 CNNè®­ç»ƒ - æ™ºèƒ½ä¼˜åŒ–ç‰ˆ')
    
    # æ•°æ®å’Œè¾“å‡º
    parser.add_argument('--data-dir', type=str, default='./data',
                        help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--out-dir', type=str, default='./runs/cifar10_optimized',
                        help='è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100,
                   help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=128,
                        help='æ‰¹æ¬¡å¤§å°ï¼ˆä¼šæ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è°ƒæ•´ï¼‰')
    parser.add_argument('--lr', type=float, default=0.1,
                        help='åŸºç¡€å­¦ä¹ ç‡ï¼ˆä¼šæ ¹æ®æ‰¹æ¬¡å¤§å°è‡ªåŠ¨ç¼©æ”¾ï¼‰')
    parser.add_argument('--weight-decay', type=float, default=5e-4,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--min-lr', type=float, default=1e-5,
                        help='æœ€å°å­¦ä¹ ç‡')
    
    # ç®—åŠ›ä¼˜åŒ–å‚æ•°
    parser.add_argument('--auto-batch-size', action='store_true', default=True,
                        help='è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--auto-optimized', action='store_true', default=True,
                        help='å¯ç”¨ç¡¬ä»¶è‡ªåŠ¨ä¼˜åŒ–')
    parser.add_argument('--disable-auto', action='store_true',
                        help='ç¦ç”¨æ‰€æœ‰è‡ªåŠ¨ä¼˜åŒ–')
    parser.add_argument('--stable-mode', action='store_true',
                        help='ç¨³å®šæ¨¡å¼ï¼šä¿å®ˆçš„å‚æ•°è®¾ç½®ï¼Œä¼˜å…ˆç¨³å®šæ€§')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--num-workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½è¿›ç¨‹æ•°ï¼ˆä¼šæ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è°ƒæ•´ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # å¤„ç†ç‰¹æ®Šæ¨¡å¼
    if args.disable_auto:
        args.auto_batch_size = False
        args.auto_optimized = False
        print("[è­¦å‘Š] å·²ç¦ç”¨æ‰€æœ‰è‡ªåŠ¨ä¼˜åŒ–ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šå‚æ•°")
    
    if args.stable_mode:
        # ç¨³å®šæ¨¡å¼ï¼šä¿å®ˆè®¾ç½®
        args.batch_size = min(args.batch_size, 256)  # é™åˆ¶æ‰¹æ¬¡å¤§å°
        args.lr = min(args.lr, 0.1)  # é™åˆ¶å­¦ä¹ ç‡
        args.auto_batch_size = False  # ç¦ç”¨è‡ªåŠ¨æ‰¹æ¬¡æœç´¢
        print("[ç¨³å®šæ¨¡å¼] ä½¿ç”¨ä¿å®ˆå‚æ•°è®¾ç½®ï¼Œä¼˜å…ˆè®­ç»ƒç¨³å®šæ€§")
    
    # è®¾ç½®éšæœºç§å­
    if args.seed > 0:
        torch.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
        import numpy as np
        import random
        np.random.seed(args.seed)
        random.seed(args.seed)
        # æ³¨æ„ï¼šå¯ç”¨benchmarkä¼šå½±å“å¯å¤ç°æ€§ï¼Œä½†ä¼šæå‡æ€§èƒ½
        if not args.auto_optimized:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    print(f"ğŸš€ å¼€å§‹CIFAR-10è®­ç»ƒ - æ™ºèƒ½ä¼˜åŒ–æ¨¡å¼")
    print(f"ğŸ“Š é…ç½®æ¦‚è§ˆ: Epochs={args.epochs}, åˆå§‹BatchSize={args.batch_size}, LR={args.lr}")
    
    # å¼€å§‹è®­ç»ƒ
    train_model(args)


if __name__ == '__main__':
    main()
